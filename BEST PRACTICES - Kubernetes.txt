** Karpenter - Force user to have 'nodeSelector'

Kubernetes Admission Controller?
    A Kubernetes Admission Controller is a gatekeeper that intercepts API requests after authentication & authorization but before the object is persisted in etcd.
    Types:
        Mutating Admission Controllers
            Examples
                Injecting sidecars (Istio)
                Adding default labels/annotations
                Setting default resource limits
                Injecting tolerations or node selectors
        Validating Admission Controllers
            Run after mutation, Cannot modify objects, Enforce policies
            Examples
                Enforce image from trusted registry
                Require resource limits
                Block privileged containers
                Enforce naming conventions

Custom Admission Controller
    Create custom mutating controller python flask app to patch JSON payload, deploy it as pod
    Create kind: MutatingWebhookConfiguration, webhook it to python service, add rules: Create-> Deployment resource


Open Policy Agent (OPA)
    General Purpose policy engine - using Rego language
    OPA Gatekeeper (comes with Custom Resource Definition CRD) -> Open Policy Agent
    ConstraintTemplate - CRD
        kind: ConstraintTemplate
            metadata:
                name: k8srequiredLabels
    Constraint - CRD
        kind: k8srequiredLabels
            metadata:
                name: pod-must-have-gk  (eg: deployment-must-have-gk ....)
    Install OPA Gatekeeper via GateKeeper YAML
        Installs Gatekeeper controller / service
        OPA Gatekeeper uses "ValidatingWebhookConfiguration" webhook
    Use cases:
        Enforce No Privileged containers
        Restrict Root User Containers
        Enforce Approved Container Registry
        Enforce Resource Requests & Limits (HPA requires these settings)
        Enforce Required Labels/Annotations


EKS
    Pre-Requisites Resources:
        VPC, Subnets (Public/Private), Route Tables, NAT Gateway + Elastic IP, Internet Gateway
    EKS Add-Ons
        Amazon Pod Identity Agent - Amazon Pod Identity allows Kubernetes pods to assume IAM roles
        Amazon EBS CSI Driver - Dynamically provision EBS volumes
        Metric Server - Provides resource metrics to Kubernetes
        AWS Distro for OpenTelemetry - Collects: Metrics, Traces, Logs
        Kube State Metrics - Exposes Kubernetes object state as metrics
        Prometheus Node Exporter - Exposes node-level metrics
        External DNS - Automatically manages Route53 records
        Secret Store CSI Driver

    EKS Resources:
        EKS Cluster
            EKS Cluster IAM Role (Policy: AmazonEKSClusterPolicy, AmazonEKSVPCResourceController)
            EKS Cluster Security Group
            EKS Cluster Network Interfaces
            EKS Cluster (resource: aws_eks_cluster)
        EKS Node Group
            EKS Node Group IAM Role (Policy: AmazonEKSWorkerNodePolicy, AmazonEKS_CNI_Policy, AmazonEC2ContainerRegistryReadOnly)
            EKS Node Group Security Group
            EKS Node Group Network Interfaces
            EKS Worker Nodes EC2 Instances (resource: aws_eks_node_group)
    # Create AWS EKS Cluster
        resource "aws_eks_cluster" "eks_cluster" {
        name     = "${local.name}-${var.cluster_name}"
        role_arn = aws_iam_role.eks_master_role.arn
        version = var.cluster_version

        vpc_config {
            subnet_ids = module.vpc.public_subnets
            endpoint_private_access = var.cluster_endpoint_private_access
            endpoint_public_access  = var.cluster_endpoint_public_access
            public_access_cidrs     = var.cluster_endpoint_public_access_cidrs    
        }

        kubernetes_network_config {
            service_ipv4_cidr = var.cluster_service_ipv4_cidr
        }
        
        # Enable EKS Cluster Control Plane Logging
        enabled_cluster_log_types = ["api", "audit", "authenticator", "controllerManager", "scheduler"]

        # Ensure that IAM Role permissions are created before and deleted after EKS Cluster handling.
        # Otherwise, EKS will not be able to properly delete EKS managed EC2 infrastructure such as Security Groups.
        depends_on = [
            aws_iam_role_policy_attachment.eks-AmazonEKSClusterPolicy,
            aws_iam_role_policy_attachment.eks-AmazonEKSVPCResourceController,
        ]
        }
    
    # Create AWS EKS Node Group - Public
        resource "aws_eks_node_group" "eks_ng_public" {
        cluster_name    = aws_eks_cluster.eks_cluster.name

        node_group_name = "${local.name}-eks-ng-public"
        node_role_arn   = aws_iam_role.eks_nodegroup_role.arn
        subnet_ids      = module.vpc.public_subnets
        #version = var.cluster_version #(Optional: Defaults to EKS Cluster Kubernetes version)    
        
        ami_type = "AL2_x86_64"  
        capacity_type = "ON_DEMAND"
        disk_size = 20
        instance_types = ["t3.medium"]
        
        
        remote_access {
            ec2_ssh_key = "eks-terraform-key"
        }

        scaling_config {
            desired_size = 1
            min_size     = 1    
            max_size     = 2
        }

        # Desired max percentage of unavailable worker nodes during node group update.
        update_config {
            max_unavailable = 1    
            #max_unavailable_percentage = 50    # ANY ONE TO USE
        }

        # Ensure that IAM Role permissions are created before and deleted after EKS Node Group handling.
        # Otherwise, EKS will not be able to properly delete EC2 Instances and Elastic Network Interfaces.
        depends_on = [
            aws_iam_role_policy_attachment.eks-AmazonEKSWorkerNodePolicy,
            aws_iam_role_policy_attachment.eks-AmazonEKS_CNI_Policy,
            aws_iam_role_policy_attachment.eks-AmazonEC2ContainerRegistryReadOnly,
        ] 

        tags = {
            Name = "Public-Node-Group"
        }
        }

    # Access the cluster
        aws eks --region us-east-1 update-kubeconfig --name hr-stag-eksdemo1

    IRSA vs PIA
        IRSA (IAM Roles for Service Accounts)
            1 - Collect EKS Cluster OpenID Connect Provider URL
            2 - Create OIDC provider in IAM
            3 - Create IAM Policy (with trust policy "AssumeRoleWithWebIdentity" for "system:serviceaccount:kube-system:<ANY CONTROLLER>"), IAM Role
            4 - Annotate Service account with IAM Role ARN
            5 - Install Add-on (Eg: EBS CSI Driver) with service-account-role-arn
            Pod → ServiceAccount (JWT) → OIDC Provider (EKS)  → AWS STS  → IAM Role  → AWS Service
        PIA (Pod Identity Agent)
            Amazon Pod Identity allows Kubernetes pods to assume IAM roles
            1 - Install EKS Pod Identity Agent (UI or CLI or Terraform 'aws_eks_addon') which runs Daemonset
            2 - Create IAM Policy (with trust policy who can assume), IAM Role
            3 - Create Pod Identity Association (Associate the Service Account with IAM Role)
            4 - Install Add-on (Eg: EBS CSI Driver) with service-account-role-arn

On-Prem Kubernetes Encryption vs EKS Encryption
    On-Prem:
        Configure 'EncryptionConfiguration' object with any Encryption algorithm with encode secret in a config file.
        Update that config file in API Server's manifest (--encryption-provider-config=config.yaml)
    EKS:
        Install AWS Secrets Manager CSI Driver Provider (aws github) using Helm in kube-system namespace => Run as Daemonset
        Install Kubernetes Secret Store CSI Driver Provider (sigs github) using Helm in kube-system namespace => Run as Daemonset
        Pod Identity Association (Map Kubernetes ServiceAccount to IAM Role)
    

aws-node vs kube-proxy
    aws-node ==> Pod Networking
        DaemonSet in kube-system
        AWS VPC CNI plugin
        Assign IP to Pods
        Attach ENIs
    kube-proxy ==> Service Networking
        DaemonSet in kube-system
        Programs service routing rules

Karpenter
    Karpenter is an open-source, Kubernetes-native node autoscaler created by AWS that provisions right-sized EC2 instances on demand for Kubernetes workloads.
    Instead of scaling node groups, Karpenter directly launches EC2 instances based on pod scheduling requirements.

    Karpenter Architecture
        Reads NodePool for (nodeSelector: on-demand / spot)
        NodePool CRD - What kind of nodes to create?
        EC2NodeClass CRD - How to create nodes 

        NodePool CRD
            Create One NodePool object for on-demand and another one for spot
            define capacity type (on-demand, spot), vm family etc in NodePool object
            Reference EC2NodeClass using "nodeClassRef:"
        EC2NodeClass CRD
            Which AMI, VPC Subnet, Security Group, IAM Role
        Karpenter controller watches NodePool objects
    Benefits
        Security
        Reliability
        Automation
    Terraform Automation
        1) Setup Carpenter Controller IAM (Karpenter Assume Role)
            Policy -> Role -> aws_eks_pod_identity_association (SA <-> Role)
        2) Setup EC2 Node IAM (Node Assume Role)
        3) Configure Event Bridge and SQS
        4) Install Karpenter

Observability
    Core Outputs:
        Metrics (CPU Usage, Request Rate, Error Rate Percentage) - WHAT is happening? ==> Amazon Managed Prometheus (AMP) & Amazon Managed Grafana (AMG)
        Logs (Time stamped records) - WHAT happened and context logs? ==> AWS CloudWatch
        Traces (Request flow across services)- WHERE and Why? ==> AWS X-Ray
    OpenTelemetry (OTel) 
        - is a vendor-neutral, open-source observability framework for collecting, processing, and exporting telemetry data (metrics, logs, and traces)
        Before OTel 
            Jaeger SDK                  -> Jaeger Server (Tracing)
            Prometheus Client Libraries -> Prometheus (Collect Metrics)
        In OTel
            OTel SDK -> Otel Collector
                        -> Exporter 1 -> Jaeger
                        -> Exporter 2 -> AWS X-Ray
    AWS Distro for OpenTelemetry (ADOT) 
        - is a secure, production-ready, AWS-supported distribution of the open-source OpenTelemetry project, providing APIs, libraries, agents, and collectors to gather 
          metrics, traces, and logs
        - Install as AWS EKS Add-On which deploy ADOT Operator (Deployment, Service, WebHook)
        - Install ADOT Collector (Can be deployed as Deployment, StatefulSet, Sidecar Container, DaemonSet)
        Flow
            Deployments (Pod with OpenTelemetry annotations) --> scrape metric -> ADOT Collector (Process it) -> Export Traces to X-Ray, Metrics to Prometheus, Logs to CloudWatch
    ADOT Installation
        Steps
            ADOT Collector IAM Policy, Role, Pod Identity Association
            Cert-Manager Add-On
            AWS Distro for OpenTelemetry (ADOT) Add-On
            Kube State Metrics Add-On (watches deployments, pods, ConfigMap etc)
            Prometheus Node Exporter Add-On (Runs as DaemonSet - monitors Worker node CPU, Disk, Network)
            Create ADOT Collector Service Account in EKS <- Cluster Role Binding <- Cluster Role
            Prometheus Workspace
            Grafana IAM Policy (Read data from Prometheus & Send notifications to SNS), IAM Role, Workspace
    Deploy Application
        Configure Annotation in the Deployment
            .Values.opentelemetry.instrumentation = "default-instrumentation"
            instrumentation.opentelemetry.io/inject-sdk: {{ .Values.opentelemetry.instrumentation }}


CSI => The Container Storage Interface (CSI) in Kubernetes is a standard API specification that allows storage vendors to develop plugins, called CSI drivers
CNI => (Container Network Interface) is a crucial specification and set of libraries in Kubernetes that standardizes how networking is configured for pods.
    AWS VPC CNI, Calico, Cilium
OCI => Open Container Initiative (OCI) standards, which are open specifications for container formats (images) and runtimes

Useful kubectl Commands:
    Verify Service Connectivity (Internal Test)
        kubectl run test --image=curlimages/curl -it --rm -- sh
        curl http://catalog-service:8080/topology

    DNS Resolution Check
        kubectl run dns-test --image=busybox:1.28 -it --rm
        nslookup catalog-service
            ==> catalog-service.default.svc.cluster.local
        